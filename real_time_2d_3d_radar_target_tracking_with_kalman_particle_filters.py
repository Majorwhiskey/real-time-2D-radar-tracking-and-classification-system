# -*- coding: utf-8 -*-
"""Real-Time 2D/3D Radar Target Tracking with Kalman/Particle Filters.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17zlLKQCayHi1UJHgWVhhV9Oa4UdARQR-
"""

# ==============================================================
# RADAR TARGET TRACKING & CLASSIFICATION PIPELINE (Improved)
# Phases: 1) KF  2) PF  3) Plots  4) Classification
# ==============================================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import (
    confusion_matrix, ConfusionMatrixDisplay, accuracy_score,
    classification_report
)

# ----------------------------------------------------------------
# CONFIG
# ----------------------------------------------------------------
np.random.seed(42)           # for reproducibility
T = 1.0                      # time step
N = 50                       # number of frames
true_vel = np.array([1.0, 0.5, 0.2])  # m/step constant velocity
process_var_base = 0.01      # KF base process noise
meas_noise_std = 2.0         # radar measurement std (meters)
pf_num_particles = 1000      # PF particle count
pf_vel_noise = 0.05          # PF velocity diffusion scale
use_rf_compare = True        # also run RandomForest comparison
do_grid_svm = True           # grid search SVM hyperparams

# ----------------------------------------------------------------
# PHASE 1: SIMULATE GROUND TRUTH & MEASUREMENTS
# ----------------------------------------------------------------
pos = np.zeros(3)
true_pos = []
for _ in range(N):
    pos = pos + true_vel
    true_pos.append(pos.copy())
true_pos = np.array(true_pos)

# Noisy radar measurements (Cartesian already)
measurements = true_pos + np.random.normal(0, meas_noise_std, true_pos.shape)

# ----------------------------------------------------------------
# PHASE 1 (cont.): 3D KALMAN FILTER TRACKING (6D CV MODEL)
# State: [x y z vx vy vz]^T, meas: [x y z]
# ----------------------------------------------------------------
F = np.block([
    [np.eye(3), T*np.eye(3)],
    [np.zeros((3,3)), np.eye(3)]
])

H = np.block([
    [np.eye(3), np.zeros((3,3))]
])

Q_base = process_var_base * np.eye(6)
R = (meas_noise_std**2) * np.eye(3)

x_est = np.zeros((6,1))  # init state
P = np.eye(6) * 100.0     # large initial uncertainty

kf_estimates = []
kf_vel_history = []  # to extract motion features later

for z in measurements:
    # Adaptive (small) jitter to Q each step to reflect maneuver uncertainty
    adaptive_Q = Q_base + np.diag(np.abs(np.random.normal(0, process_var_base*0.5, size=6)))

    # Predict
    x_pred = F @ x_est
    P_pred = F @ P @ F.T + adaptive_Q

    # Update
    z = z.reshape(3,1)
    y = z - H @ x_pred
    S = H @ P_pred @ H.T + R
    K = P_pred @ H.T @ np.linalg.inv(S)
    x_est = x_pred + K @ y
    P = (np.eye(6) - K @ H) @ P_pred

    kf_estimates.append(x_est[:3].flatten())
    kf_vel_history.append(x_est[3:6].flatten())

kf_estimates = np.array(kf_estimates)
kf_vel_history = np.array(kf_vel_history)

# KF error metrics
kf_errors = kf_estimates - true_pos
kf_rmse = np.sqrt(np.mean(kf_errors**2, axis=1))  # per-step
kf_rmse_total = np.sqrt(np.mean(kf_errors**2))    # overall


# ----------------------------------------------------------------
# PHASE 2: PARTICLE FILTER (3D) WITH SYSTEMATIC RESAMPLING
# State per particle: [x y z vx vy vz]
# ----------------------------------------------------------------
def systematic_resample(weights: np.ndarray) -> np.ndarray:
    Np = len(weights)
    positions = (np.arange(Np) + np.random.rand()) / Np
    cumsum = np.cumsum(weights)
    idx = np.zeros(Np, dtype=int)
    i = j = 0
    while i < Np:
        if positions[i] < cumsum[j]:
            idx[i] = j
            i += 1
        else:
            j += 1
    return idx

# init particles around first measurement
particles = np.zeros((pf_num_particles, 6))
particles[:, :3] = measurements[0] + np.random.normal(0, 5.0, (pf_num_particles, 3))
particles[:, 3:] = np.random.normal(0, 1.0, (pf_num_particles, 3))
weights = np.ones(pf_num_particles) / pf_num_particles

pf_estimates = []
pf_vel_history = []

for t in range(N):
    # Prediction: constant velocity + adaptive noise
    # Increase motion_noise when particle spread is small to avoid degeneracy
    spread = np.mean(np.std(particles[:, :3], axis=0))
    motion_noise_pos = np.clip(spread*0.05, 0.01, 1.5)
    particles[:, :3] += particles[:, 3:]*T + np.random.normal(0, motion_noise_pos, (pf_num_particles, 3))
    particles[:, 3:] += np.random.normal(0, pf_vel_noise, (pf_num_particles, 3))

    # Update: measurement likelihood
    z = measurements[t]
    dists = np.linalg.norm(particles[:, :3] - z, axis=1)
    weights = np.exp(-0.5 * (dists / meas_noise_std)**2)
    weights += 1e-300
    weights /= np.sum(weights)

    # Resample
    res_idx = systematic_resample(weights)
    particles = particles[res_idx]
    weights = np.ones_like(weights) / pf_num_particles

    # Estimate (weighted mean -- uniform after resample so just mean)
    est_pos = np.mean(particles[:, :3], axis=0)
    est_vel = np.mean(particles[:, 3:], axis=0)
    pf_estimates.append(est_pos)
    pf_vel_history.append(est_vel)

pf_estimates = np.array(pf_estimates)
pf_vel_history = np.array(pf_vel_history)

# PF error metrics
pf_errors = pf_estimates - true_pos
pf_rmse = np.sqrt(np.mean(pf_errors**2, axis=1))
pf_rmse_total = np.sqrt(np.mean(pf_errors**2))


# ----------------------------------------------------------------
# PHASE 3: PLOTS (KF vs PF, ERROR CURVES)
# ----------------------------------------------------------------
fig = plt.figure(figsize=(14,6))

# KF plot
ax1 = fig.add_subplot(121, projection='3d')
ax1.plot(*true_pos.T, label='True Position', color='blue')
ax1.plot(*measurements.T, 'rx', label='Radar Measurements')
ax1.plot(*kf_estimates.T, label='Kalman Estimate', color='green')
ax1.set_title(f"3D Kalman Filter Tracking\nRMSE={kf_rmse_total:.2f}")
ax1.set_xlabel('X'); ax1.set_ylabel('Y'); ax1.set_zlabel('Z')
ax1.legend()

# PF plot
ax2 = fig.add_subplot(122, projection='3d')
ax2.plot(*true_pos.T, label='True Position', color='blue')
ax2.plot(*measurements.T, 'rx', label='Radar Measurements')
ax2.plot(*pf_estimates.T, label='Particle Filter Estimate', color='purple')
ax2.set_title(f"3D Particle Filter Tracking\nRMSE={pf_rmse_total:.2f}")
ax2.set_xlabel('X'); ax2.set_ylabel('Y'); ax2.set_zlabel('Z')
ax2.legend()

plt.tight_layout()
plt.show()

# Error over time
plt.figure(figsize=(8,4))
plt.plot(kf_rmse, label='KF RMSE per step')
plt.plot(pf_rmse, label='PF RMSE per step')
plt.xlabel("Time Step")
plt.ylabel("RMSE (m)")
plt.title("Per-step Tracking Error")
plt.grid(True)
plt.legend()
plt.show()


# ----------------------------------------------------------------
# PHASE 4: FEATURE EXTRACTION FOR CLASSIFICATION
# We'll build a synthetic dataset of multiple tracks (half slow, half fast)
# preserving the feature pipeline: we extract motion statistics -> classify
# ----------------------------------------------------------------

def simulate_track(label, steps=40):
    """
    label = 0 (slow) or 1 (fast). Generate synthetic 3D motion track.
    Returns dict with 'positions', 'label'.
    """
    # base speed scale
    if label == 0:
        base_v = np.array([0.8, 0.3, 0.1]) + np.random.randn(3)*0.1
        accel_mag = 0.05
    else:
        base_v = np.array([2.0, 1.2, 0.6]) + np.random.randn(3)*0.3
        accel_mag = 0.2

    p = np.zeros(3)
    v = base_v.copy()
    positions = []
    for _ in range(steps):
        # small random accel wiggle
        a = np.random.randn(3) * accel_mag
        v += a
        p += v
        positions.append(p.copy())
    return {'positions': np.array(positions), 'label': label}


def extract_motion_features_from_positions(positions: np.ndarray):
    """
    Given Nx3 positions, return engineered motion features:
    avg_speed, max_speed, avg_accel, max_accel, jerk_mean, path_length,
    straightness_ratio, curvature_mean, turn_rate_std.
    """
    # velocities
    vel = np.diff(positions, axis=0)
    if vel.shape[0] == 0:
        return np.zeros(9)
    speed = np.linalg.norm(vel, axis=1)

    # accelerations (scalar based on speed change)
    accel = np.diff(speed)

    # jerk
    jerk = np.diff(accel) if accel.shape[0] > 0 else np.array([0.0])

    # path metrics
    seg_lengths = np.linalg.norm(np.diff(positions, axis=0), axis=1)
    path_length = np.sum(seg_lengths)
    straight_line = np.linalg.norm(positions[-1] - positions[0])
    straightness_ratio = straight_line / (path_length + 1e-9)

    # curvature approx from successive direction changes
    dirs = vel / (speed[:,None] + 1e-9)
    dir_diff = np.diff(dirs, axis=0)
    curvature = np.linalg.norm(dir_diff, axis=1)

    # turn rate (change in heading angle in XY)
    headings = np.arctan2(vel[:,1], vel[:,0])
    turn_rate = np.diff(headings)

    feat = [
        np.mean(speed),         # avg_speed
        np.max(speed),          # max_speed
        np.mean(accel) if accel.size else 0.0,  # avg_accel
        np.max(accel) if accel.size else 0.0,   # max_accel
        np.mean(jerk) if jerk.size else 0.0,    # jerk_mean
        path_length,
        straightness_ratio,
        np.mean(curvature) if curvature.size else 0.0,
        np.std(turn_rate) if turn_rate.size else 0.0
    ]
    return np.array(feat, dtype=float)


# Build dataset
num_tracks = 300   # total synthetic samples
tracks = []
for _ in range(num_tracks//2):
    tracks.append(simulate_track(0, steps=np.random.randint(20,60)))
for _ in range(num_tracks//2):
    tracks.append(simulate_track(1, steps=np.random.randint(20,60)))

# Extract features
X_list, y_list = [], []
for tr in tracks:
    X_list.append(extract_motion_features_from_positions(tr['positions']))
    y_list.append(tr['label'])
X = np.vstack(X_list)
y = np.array(y_list)

# ----------------------------------------------------------------
# TRAIN / TEST SPLIT & SCALING
# ----------------------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_test_s  = scaler.transform(X_test)

# ----------------------------------------------------------------
# CLASSIFIER 1: SVM (with class_weight + optional grid search)
# ----------------------------------------------------------------
if do_grid_svm:
    param_grid = {
        'C': [0.1, 1, 10, 100],
        'gamma': ['scale', 'auto'],
        'kernel': ['rbf', 'linear']
    }
    svm = SVC(class_weight='balanced')
    clf_svm = GridSearchCV(svm, param_grid, cv=5, n_jobs=-1)
    clf_svm.fit(X_train_s, y_train)
    print("Best SVM params:", clf_svm.best_params_)
    svm_model = clf_svm.best_estimator_
else:
    svm_model = SVC(kernel='rbf', class_weight='balanced')
    svm_model.fit(X_train_s, y_train)

y_pred_svm = svm_model.predict(X_test_s)
acc_svm = accuracy_score(y_test, y_pred_svm)
print(f"\nSVM Accuracy: {acc_svm*100:.2f}%")
print("\nSVM Classification Report:")
print(classification_report(y_test, y_pred_svm, target_names=['Slow','Fast']))

cm_svm = confusion_matrix(y_test, y_pred_svm)
disp_svm = ConfusionMatrixDisplay(confusion_matrix=cm_svm, display_labels=['Slow','Fast'])
disp_svm.plot(cmap=plt.cm.Blues)
plt.title(f"SVM Target Classification - Acc: {acc_svm*100:.2f}%")
plt.show()


# ----------------------------------------------------------------
# CLASSIFIER 2 (optional compare): Random Forest
# ----------------------------------------------------------------
if use_rf_compare:
    rf_model = RandomForestClassifier(
        n_estimators=200,
        max_depth=None,
        random_state=42,
        class_weight='balanced'  # help with any imbalance
    )
    rf_model.fit(X_train, y_train)  # tree models don't strictly need scaling
    y_pred_rf = rf_model.predict(X_test)
    acc_rf = accuracy_score(y_test, y_pred_rf)
    print(f"\nRandom Forest Accuracy: {acc_rf*100:.2f}%")
    print("\nRandom Forest Classification Report:")
    print(classification_report(y_test, y_pred_rf, target_names=['Slow','Fast']))

    cm_rf = confusion_matrix(y_test, y_pred_rf)
    disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=['Slow','Fast'])
    disp_rf.plot(cmap=plt.cm.Greens)
    plt.title(f"Random Forest Target Classification - Acc: {acc_rf*100:.2f}%")
    plt.show()

    # Feature importance (RF only)
    rf_importances = rf_model.feature_importances_
    feat_names = [
        'avg_speed','max_speed','avg_accel','max_accel','jerk_mean',
        'path_len','straightness','curvature','turn_rate_std'
    ]
    plt.figure(figsize=(8,4))
    plt.bar(feat_names, rf_importances)
    plt.xticks(rotation=45, ha='right')
    plt.ylabel('Importance')
    plt.title('Random Forest Feature Importance')
    plt.tight_layout()
    plt.show()


# ----------------------------------------------------------------
# OPTIONAL: USE YOUR KF / PF TRACKS AS ONE MORE SAMPLE
# (Demonstrates extracting features from your actual tracking run.)
# ----------------------------------------------------------------
kf_feat = extract_motion_features_from_positions(kf_estimates)
pf_feat = extract_motion_features_from_positions(pf_estimates)
kf_pred_label = svm_model.predict(scaler.transform(kf_feat.reshape(1,-1)))[0]
pf_pred_label = svm_model.predict(scaler.transform(pf_feat.reshape(1,-1)))[0]

print("\nPredicted label for KF track (0=Slow,1=Fast):", kf_pred_label)
print("Predicted label for PF track (0=Slow,1=Fast):", pf_pred_label)